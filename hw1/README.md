# Homework 1: Project Gutenberg command-line wrangling 
### CS186, UC Berkeley, Spring 2015

## Description

This is a modified version of the 2015 task as the required input and example files from https://inst.eecs.berkeley.edu/ were not available.

### Your tools
For this assignment, you are limited to using [Python](https://www.python.org/), [bash](http://www.gnu.org/software/bash/), and the [standard Unix utilities](http://en.wikipedia.org/wiki/List_of_Unix_utilities).

### Your constraints
You need to be able to handle an input file that is far larger than the memory of the computer that runs the script.  To do so, you should:

1. write streaming Python and bash code that only requires a fraction of the data to be in memory at a time, and
2. utilize UNIX utilities like `sort` that provide out-of-core divide-and-conquer algorithms.

You should not need to write very complicated code in either Python or bash.  Take advantage of UNIX utilities as much as you can.  In particular, note that there is no need for you to write an out-of-core algorithm to complete this homework: UNIX utilities can do the heavy lifting if you orchestrate them properly.

## Getting Started

The hw1 directory should look like this:

    ebook.csv  name_counts.csv    README.md  token_counts.csv	books/
    hw1.sh     popular_names.txt  test       tokens.csv
	
In addition to this README file, you will see:

* `hw1.sh`, a skeleton of the bash file you will write
* `test`, a subdir with python unit tests you can use to validate your solution,
* `popular_names.txt`, a list of popular names to look for in the tokens,
* `ebook.csv`, `name_counts.csv`, `token_counts.csv` and `tokens.csv`:
   correctly-formatted example outputs.
* `books/`, directory with some txt gutenberg files of popular books, some long some shorter.

## Specification
Your solution should be driven by a `hw1.sh` script that is passed one argument: a .txt file that contains a concatenation of valid ebooks:

    vagrant@precise64:~$ ./hw1.sh ebooks_tiny.txt

The script should overwrite the four example csv output files, as follows:

* `ebook.csv` should be a legal csv file containing the same header row as the example. Take a look at a few of the ebooks to determine how to best parse the necessary fields. 
    * If the ebook title is more than one line, just take the first line as the title. 
    * The ebook body should only include text from the start of the actual ebook (not including the headers added by Gutenberg Project). The start of the ebook's body is indicated by a line beginning with the string `*** START OF THE PROJECT GUTENBERG`, and the end of the body is indicated by a line beginning with `*** END OF THE PROJECT GUTENBERG`.
    * If one of the fields is not available for an ebook, the entry should be "null".
    * Depending on how you parse the fields, be sure to strip the carriage return and line feed (\\r\\n) at the end of the entries, except for the body.
* `tokens.csv` should be a csv file with the same header row as the example: `ebook_id,token`.  This file is generated by taking the `body` field of an ebook, and splitting it on non-alphabetical characters into separate tokens (substrings) which are converted to all-lowercase characters.  After splitting and lowercasing, each token should be copied into the `tokens.csv` file, prepended by the associated `ebook_id` (and a comma).  *Note that a given token may appear multiple times per ebook\_id, and/or multiple times across different ebook\_id  s.*
* `token_counts.csv` should have the same header row as the example, and sum up the number of occurrences of each *distinct* token in the `tokens.csv` file.
* `name_counts.csv` is intended to store the rough result of the question "how often is each name mentioned in all of the books?" It should have the same header as the example file, and then contain those rows from `token_counts.csv` with the (lowercase) name in the `token` field. For simplicity, we will only be looking at the top 50 most popular names for boys and girls. These names are provided in `popular_names.txt`.

As in the example csvs, the line endings for your output csvs should be CRLF (\\r\\n). This is the default line ending for Python's csvwriter.

### Testing
A simple [Python unit test] is provided in `test_ebook.py`.  If your code is working, you should see something like this if you type the first line to a bash shell in the hw1 directory:

    vagrant@precise64:~/course/hw1$ python test/ebook_test.py TestEbook.test_sanity
    .
    ----------------------------------------------------------------------
    Ran 1 test in 10.550s

    OK
    vagrant@precise64:~/course/hw1$

The sanity test runs your `hw1.sh` script against a handful of ebooks taken from the Gutenberg Project, and compares your output to what the solution produced.  You should have a look at the unit test files -- they are simple and you'd be wise to understand what they're checking.

Our grading script will compare your code against approximately 1100 ebooks provided on the Gutenberg Project website.  (We may also test against other data.)  To test against the full data set, you type:

    vagrant@precise64:~/course/hw1$ python test/ebook_test.py TestEbook.test_full
    .
    ----------------------------------------------------------------------
    Ran 1 test in 85.789s

    OK
    vagrant@precise64:~/course/hw1$

and hopefully you get 0-failure/0-error output similar to the listing above. 

To run both tests, you simply type:

    vagrant@precise64:~$ ./test/ebook_test.py 

We need to ensure that your code will scale to data sets that are bigger than memory -- no matter how much memory is on your test machine.  To this end, the test scripts use Python's [setrlimit](https://docs.python.org/2/library/resource.html) command to cap the amount of virtual memory your hw1.sh script allocates.  If you get a Segmentation fault error, then your code is not doing appropriate streaming and/or divide-and-conquer!

## Notes
* As noted in `hw1.sh`, the last line should say "exit 0" to indicate a successful completion.  This is important for making the tests run correctly!
* Consider using `sys.stdin`, `sys.stdout`, and `sys.stderr` in Python, and [UNIX pipes](http://en.wikipedia.org/wiki/Pipeline_(Unix)) to put together separate scripts.  method for writing to csv files.
* Python has a handy [CSV library](https://docs.python.org/2/library/csv.html) for csv manipulation and string manipulation.  It will make your life simpler.
* The UNIX utilties are written in C and are faster than anything you will write in Python.  So if your code seems very slow and you want to speed it up, try to use less Python and work more with the UNIX utilities.  Your final solution should complete the `test_full` test in a few minutes on the VM.  (The 85 second number above was from a fairly recent MacBook Pro with only lightly tuned code.  Your mileage may vary.)
* If using Python's CSV reader to read csvs, the body fields may be too large for Python's current field limit, even though they should certainly fit in memory. To fix that, change the field size limit:
    % csv.field_size_limit(sys.maxint)